# ğŸš€ RAG-Pipeline for Intelligent Recommendations on Reddit

This project consists of three main components for extracting, embedding, and analyzing Reddit data.

## ğŸ“Œ Components

### 1ï¸âƒ£ `extractor.py` - Reddit Data Extractor ğŸ“

**Purpose:** Scrapes data from Reddit using PRAW (Reddit API wrapper)

#### âœ¨ Key Features:

- ğŸ“¤ Extracts posts, comments, and user information from specified subreddits
- â³ Handles rate limiting with exponential backoff
- ğŸ§¹ Cleans and normalizes text content
- ğŸ”„ Processes comments recursively while maintaining thread structure
- âš¡ Uses threading for parallel processing
- ğŸ’¾ Saves data in JSON format

### 2ï¸âƒ£ `embeddings.py` - Data Embedding & Storage ğŸ“Š

**Purpose:** Processes the extracted Reddit data and stores it in a vector database (ChromaDB)

#### âœ¨ Key Features:

- ğŸ§  Uses HuggingFace embeddings model (`sentence-transformers`)
- âš™ï¸ Handles documents asynchronously for better performance
- ğŸ”„ Processes posts, comments, and user data in batches
- ğŸ”‘ Generates unique IDs for each document
- ğŸ“‚ Maintains metadata for all content types
- ğŸš€ Uses `ThreadPoolExecutor` for parallel processing

### 3ï¸âƒ£ `app.py` - Streamlit Web Interface ğŸ¨

**Purpose:** Provides a user interface to analyze the processed Reddit data

#### âœ¨ Key Features:

- ğŸ·ï¸ Three main modes:
  1. **User Details:** ğŸ§ Analyze specific Reddit users
  2. **Topic Exploration:** ğŸ” Find users discussing specific topics
  3. **Content Synthesis:** ğŸ¤– AI-powered analysis of content
- ğŸ” Uses ChromaDB for similarity search
- ğŸ¤ Integrates with Ollama for content synthesis
- âš¡ Caches embeddings for better performance
- ğŸ›ï¸ Rich UI with expandable sections, metrics, and tabs

## ğŸ› ï¸ Installation

### ğŸ“‹ Prerequisites

- ğŸ Python 3.8+
- ğŸ”— PRAW
- ğŸ›ï¸ ChromaDB
- ğŸ§  HuggingFace `sentence-transformers`
- ğŸŒ Streamlit
- ğŸ¤– Ollama

### âš™ï¸ Setup

```bash
# Clone the repository
git clone https://github.com/your-repo/reddit-data-pipeline.git
cd reddit-data-pipeline

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows use `venv\Scripts\activate`

# Install dependencies
pip install -r requirements.txt

# Configure Reddit API Credentials
export REDDIT_CLIENT_ID="your_client_id"
export REDDIT_CLIENT_SECRET="your_client_secret"
export REDDIT_USER_AGENT="your_user_agent"
```

## ğŸš€ Usage

### ğŸ“ Extract Reddit Data

```bash
python extractor.py --subreddit example_subreddit --limit 1000
```

### ğŸ”„ Process & Embed Data

```bash
python embeddings.py --input data.json
```

### ğŸ¨ Run Web Interface

```bash
streamlit run app.py
```

## ğŸ“œ License

This project is licensed under the MIT License.

---

**ğŸ‘¨â€ğŸ’» Author:** Kunal Chakane\
**ğŸ“§ Contact:** [kunalchakane07@gmail.com](mailto\:kunalchakane07@gmail.com)

